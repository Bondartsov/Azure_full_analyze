import tiktoken
from tqdm import tqdm
import os
from core.azure.repos import get_repo_files, get_file_content
from core.logging.logger import log
from dotenv import load_dotenv  # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

# –ü–æ–ª—É—á–µ–Ω–∏–µ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
WHITE_EXTENSIONS = set(os.getenv("WHITE_EXTENSIONS", "").split(","))

def count_tokens_in_repo(project_name, repository_name):
    """
    –°—á–∏—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã, —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ —Ñ–∞–π–ª–∞—Ö (—É –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ WHITE_EXTENSIONS).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (files_data, total_tokens).
    files_data -> [{"path": ..., "tokens": int, "lines": int, "comments": int}, ...]
    """
    total_tokens = 0
    files_data = []
    log(f"üìä –ù–∞—á–∞–ª–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤, —Å—Ç—Ä–æ–∫ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ {repository_name} (–±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫).")

    files = get_repo_files(project_name, repository_name)
    if not files:
        log(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è {repository_name}", level="WARNING")
        return [], 0

    for file_path in tqdm(files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤"):
        _, ext = os.path.splitext(file_path.lower())
        if ext not in WHITE_EXTENSIONS:
            continue

        content = get_file_content(project_name, repository_name, file_path)
        if not content.strip():
            continue

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        tokens_count = count_tokens_in_text(content)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)
        lines_count = content.count('\n') + 1

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–Ω–∞–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        comments_count = count_comments_naive(content, ext)

        files_data.append({
            "path": file_path,
            "tokens": tokens_count,
            "lines": lines_count,
            "comments": comments_count,
        })
        total_tokens += tokens_count

    return files_data, total_tokens

def count_tokens_in_text(text, model_encoding="cl100k_base"):
    encoding = tiktoken.get_encoding(model_encoding)
    return len(encoding.encode(text))

def count_comments_naive(content, ext):
    """
    –ù–∞–∏–≤–Ω–æ —Å—á–∏—Ç–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è —Ä—è–¥–∞ —è–∑—ã–∫–æ–≤:
    - C-like (C#, C++, Java, JS): –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ //, /*...*/
    - Python: —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å #
    –∏ —Ç.–¥.
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    lines = content.split('\n')
    comment_lines = 0
    in_block_comment = False  # –¥–ª—è /* ... */
    
    for line in lines:
        stripped = line.strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python-style #
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ ext == '.py', —Ç–æ –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å # –∫–∞–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        if ext == '.py':
            if stripped.startswith('#'):
                comment_lines += 1
                continue

        # C++/Java/C#/JS single line //
        if stripped.startswith('//'):
            comment_lines += 1
            continue

        # –ë–ª–æ–∫–æ–≤—ã–µ /* ... */
        if '/*' in stripped:
            in_block_comment = True
            comment_lines += 1  # —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –≤—Å—è —Å—Ç—Ä–æ–∫–∞ - –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
            if '*/' in stripped and stripped.index('/*') < stripped.index('*/'):
                # –ï—Å–ª–∏ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è
                in_block_comment = False
            continue
        if in_block_comment:
            comment_lines += 1
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏
            if '*/' in stripped:
                in_block_comment = False
            continue

    return comment_lines
