import tiktoken
from tqdm import tqdm
import os
from core.azure.repos import get_repo_files, get_file_content
from core.logging.logger import log
from dotenv import load_dotenv  # –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

# –ü–æ–ª—É—á–µ–Ω–∏–µ –±–µ–ª–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
WHITE_EXTENSIONS = set(os.getenv("WHITE_EXTENSIONS", "").split(","))


def count_tokens_in_repo(project_name, repository_name):
    """
    –°—á–∏—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã, —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ —Ñ–∞–π–ª–∞—Ö (—É –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ WHITE_EXTENSIONS).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (files_data, total_tokens).
    files_data -> [{"path": ..., "tokens": int, "lines": int, "comments": int}, ...]
    """
    total_tokens = 0
    files_data = []
    log(f"üìä –ù–∞—á–∞–ª–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤, —Å—Ç—Ä–æ–∫ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ {repository_name} (–±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫).")

    files = get_repo_files(project_name, repository_name)
    if not files:
        log(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è {repository_name}", level="WARNING")
        return [], 0

    for file_path in tqdm(files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤"):
        _, ext = os.path.splitext(file_path.lower())
        if ext not in WHITE_EXTENSIONS:
            continue

        content = get_file_content(project_name, repository_name, file_path)
        if not content.strip():
            continue

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        tokens_count = count_tokens_in_text(content)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)
        lines_count = content.count('\n') + 1

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–Ω–∞–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        comments_count = count_comments_naive(content, ext)

        files_data.append({
            "path": file_path,
            "tokens": tokens_count,
            "lines": lines_count,
            "comments": comments_count,
        })
        total_tokens += tokens_count

    return files_data, total_tokens


def count_tokens_in_text(text, model="o3-mini"):
    """
    –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º tiktoken.
    –î–ª—è –º–æ–¥–µ–ª–µ–π, –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —è–≤–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞.
    """
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —è–≤–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
        log(f"‚ùó –ú–æ–¥–µ–ª—å '{model}' –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ 'cl100k_base'.")
        encoding = tiktoken.get_encoding("cl100k_base")

    tokens = encoding.encode(text)
    return len(tokens)


def split_text(text, max_tokens=3000, model="o3-mini"):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_tokens.
    """
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        log(f"‚ùó –ú–æ–¥–µ–ª—å '{model}' –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ 'cl100k_base'.")
        encoding = tiktoken.get_encoding("cl100k_base")

    tokens = encoding.encode(text)
    parts = []
    current_tokens = []

    for token in tokens:
        current_tokens.append(token)
        if len(current_tokens) >= max_tokens:
            parts.append(encoding.decode(current_tokens))
            current_tokens = []

    if current_tokens:
        parts.append(encoding.decode(current_tokens))

    return parts


def count_comments_naive(content, ext):
    """
    –ù–∞–∏–≤–Ω–æ —Å—á–∏—Ç–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è —Ä—è–¥–∞ —è–∑—ã–∫–æ–≤:
    - C-like (C#, C++, Java, JS): –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ //, /*...*/
    - Python: —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å #
    –∏ —Ç.–¥.
    """
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    lines = content.split('\n')
    comment_lines = 0
    in_block_comment = False  # –¥–ª—è /* ... */

    for line in lines:
        stripped = line.strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python-style #
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ ext == '.py', —Ç–æ –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å # –∫–∞–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        if ext == '.py':
            if stripped.startswith('#'):
                comment_lines += 1
                continue

        # C++/Java/C#/JS single line //
        if stripped.startswith('//'):
            comment_lines += 1
            continue

        # –ë–ª–æ–∫–æ–≤—ã–µ /* ... */
        if '/*' in stripped:
            in_block_comment = True
            comment_lines += 1  # —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –≤—Å—è —Å—Ç—Ä–æ–∫–∞ - –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
            if '*/' in stripped and stripped.index('/*') < stripped.index('*/'):
                # –ï—Å–ª–∏ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è
                in_block_comment = False
            continue
        if in_block_comment:
            comment_lines += 1
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏
            if '*/' in stripped:
                in_block_comment = False
            continue

    return comment_lines

