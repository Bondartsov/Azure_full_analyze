import tiktoken
from core.azure.repos import get_repo_files, get_file_content
from core.logging.logger import log
from tqdm import tqdm

# –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
EXCLUDE_EXTENSIONS = {
    ".jpg", ".jpeg", ".png", ".gif", ".svg", ".bmp", ".tiff", ".ico",  # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    ".mp4", ".avi", ".mov", ".mkv", ".flv", ".wmv", ".webm",  # –í–∏–¥–µ–æ
    ".mp3", ".wav", ".ogg", ".flac", ".aac", ".wma",  # –ê—É–¥–∏–æ
    ".ttf", ".otf", ".woff", ".woff2",  # –®—Ä–∏—Ñ—Ç—ã
    ".bin", ".dylib", ".so", ".o", ".dll",  # –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã
    ".zip", ".tar", ".gz", ".bz2", ".7z",  # –ê—Ä—Ö–∏–≤—ã
    ".ckpt", ".pb", ".pt", ".onnx",  # –ú–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
    ".sqlite", ".db", ".sql", ".mdb", ".accdb"  # –§–∞–π–ª—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
}

def count_tokens_in_repo(project_name, repository_name):
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏, –∏—Å–∫–ª—é—á–∞—è –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã."""
    total_tokens = 0
    token_data = {}
    excluded_files = 0

    log(f"üìä –ù–∞—á–∞–ª–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ {repository_name}...")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
    files = get_repo_files(project_name, repository_name)
    if not files:
        log(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è {repository_name}.", level="WARNING")
        return {}, 0

    for file_path in tqdm(files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤"):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        if any(file_path.lower().endswith(ext) for ext in EXCLUDE_EXTENSIONS):
            log(f"‚ö† –§–∞–π–ª {file_path} –∏—Å–∫–ª—é—á—ë–Ω (–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç)")
            excluded_files += 1
            continue

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        content = get_file_content(project_name, repository_name, file_path)
        if not content.strip():
            log(f"‚ö† –§–∞–π–ª {file_path} –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å")
            continue

        # –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤
        tokens = count_tokens_in_text(content)
        token_data[file_path] = tokens
        total_tokens += tokens
        log(f"üìÑ –§–∞–π–ª {file_path} ‚Üí {tokens} —Ç–æ–∫–µ–Ω–æ–≤")

    log(f"‚úÖ DEBUG: –í —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ **{repository_name}** –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ **{len(token_data)} —Ñ–∞–π–ª–∞(–æ–≤)**, –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: **{total_tokens}**")
    
    return token_data, total_tokens

def count_tokens_in_text(text, model_encoding="cl100k_base"):
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    encoding = tiktoken.get_encoding(model_encoding)
    return len(encoding.encode(text))
