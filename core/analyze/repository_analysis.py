# core/analyze/repository_analysis.py
import os
from core.reports.generate import generate_report
from core.utils.cache import load_repo_data_from_cache, save_repo_data_to_cache
from core.utils.token_counter import count_tokens_in_repo
from core.logging.logger import log
from core.ai.report_generator import (
    generate_ai_report,
    generate_deep_report_for_repo,
    get_deep_reports_for_repo
)

def analyze_repository(project_name, repository, repo_changed, analysis_mode="fast"):
    """
    –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.
      - –ï—Å–ª–∏ analysis_mode == "fast" –∏ repo_changed == False, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å –∫—ç—à.
      - –ï—Å–ª–∏ analysis_mode == "deep" –∏–ª–∏ –∫—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
    """
    repository_name = repository.name

    if analysis_mode == "fast" and not repo_changed:
        cached_data = load_repo_data_from_cache(project_name, repository_name)
        if cached_data:
            total_tokens = cached_data.get("total_tokens", 0)
            files_data = cached_data.get("files", [])
            report_path = generate_report(project_name, repository_name, files_data)
            if report_path:
                log(f"üìÑ –û—Ç—á—ë—Ç –∞–Ω–∞–ª–∏–∑–∞ {repository_name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω (–∏–∑ –∫—ç—à–∞): {report_path}")
                return {
                    "repository": repository_name,
                    "tokens": total_tokens,
                    "cached": True,
                    "files": files_data,
                    "report_path": report_path
                }
            else:
                log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –∏–∑ –∫—ç—à–∞ –¥–ª—è {repository_name}!", level="ERROR")
                return None

    return analyze_repository_from_scratch(project_name, repository.name, analysis_mode)

def analyze_repository_from_scratch(project_name, repository_name, analysis_mode="fast"):
    """
    –°—á–∏—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –∑–∞–Ω–æ–≤–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –∏ (–ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ) –ò–ò‚Äë–æ—Ç—á—ë—Ç—ã,
    –∑–∞—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à–µ (–ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –∞–Ω–∞–ª–∏–∑–µ).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞.
    """
    token_result = count_tokens_in_repo(project_name, repository_name)
    if not token_result or not isinstance(token_result, tuple) or len(token_result) != 2:
        log("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç count_tokens_in_repo!", level="ERROR")
        return None

    files_data, total_tokens = token_result

    if analysis_mode == "fast":
        report_path = generate_report(project_name, repository_name, files_data)
    elif analysis_mode == "deep":
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ò–ò‚Äë–æ—Ç—á—ë—Ç
        report_path = generate_deep_report_for_repo(project_name, repository_name, files_data)
        try:
            with open(report_path, "r", encoding="utf-8") as f:
                aggregated_content = f.read()
            print("Aggregated Deep Analysis Report:\n", aggregated_content, flush=True)
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞: {e}", level="ERROR")
    else:
        report_path = generate_report(project_name, repository_name, files_data)

    if not report_path:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –¥–ª—è {repository_name}", level="ERROR")
        return None

    if analysis_mode == "fast":
        from core.utils.cache import save_repo_data_to_cache
        save_repo_data_to_cache(project_name, repository_name, total_tokens, files_data)
    
    log(f"üìÑ –û—Ç—á—ë—Ç –∞–Ω–∞–ª–∏–∑–∞ {repository_name} —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")

    result = {
        "repository": repository_name,
        "tokens": total_tokens,
        "cached": False,  # –ê–Ω–∞–ª–∏–∑ —Å –Ω—É–ª—è ‚Äì –∫—ç—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        "files": files_data,
        "report_path": report_path
    }

    if analysis_mode == "deep":
        deep_reports = get_deep_reports_for_repo(project_name, repository_name, files_data)
        result["ai_reports"] = deep_reports

    return result
def generate_deep_report_for_repo(project_name, repository_name, files_data):
    """
    –î–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ò–ò‚Äë–æ—Ç—á—ë—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.
    –°–æ–±–∏—Ä–∞–µ—Ç –ø—É—Ç–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –æ—Ç—á—ë—Ç–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç.
    –ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è –≤ –∫–æ–Ω—Å–æ–ª—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –æ—Ç—á—ë—Ç—É.
    """
    # –õ–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
    from core.ai.report_generator import generate_ai_report
    deep_report_paths = []
    for file_info in files_data:
        # –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ file_info —Å–æ–¥–µ—Ä–∂–∏—Ç "file_name", "folder" –∏ "content"
        file_name = file_info.get("file_name")
        folder = file_info.get("folder", "root")
        file_content = file_info.get("content")
        if not file_content:
            continue
        try:
            report_path = generate_ai_report(project_name, repository_name, folder, file_name, file_content)
            deep_report_paths.append(report_path)
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ò–ò‚Äë–æ—Ç—á—ë—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {file_name}: {e}", level="ERROR")
    
    aggregated_dir = os.path.join("ai_reports", project_name, repository_name)
    os.makedirs(aggregated_dir, exist_ok=True)
    aggregated_report_path = os.path.join(aggregated_dir, "aggregated_deep_report.txt")
    try:
        with open(aggregated_report_path, "w", encoding="utf-8") as f:
            header = f"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ò–ò‚Äë–∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è {repository_name}\n\n"
            f.write(header)
            for path in deep_report_paths:
                f.write(f"{path}\n")
        # –í—ã–≤–æ–¥ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
        print(header, flush=True)
        with open(aggregated_report_path, "r", encoding="utf-8") as f:
            print(f.read(), flush=True)
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ò–ò‚Äë–æ—Ç—á—ë—Ç–∞: {e}", level="ERROR")
    return os.path.abspath(aggregated_report_path)

def get_deep_reports_for_repo(project_name, repository_name, files_data):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø—É—Ç–µ–π –∫ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –ò–ò‚Äë–æ—Ç—á—ë—Ç–∞–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞.
    """
    from core.ai.report_generator import generate_ai_report
    deep_reports = []
    for file_info in files_data:
        file_name = file_info.get("file_name")
        folder = file_info.get("folder", "root")
        file_content = file_info.get("content")
        if not file_content:
            continue
        try:
            report_path = generate_ai_report(project_name, repository_name, folder, file_name, file_content)
            deep_reports.append(os.path.abspath(report_path))
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ò–ò‚Äë–æ—Ç—á—ë—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {file_name}: {e}", level="ERROR")
    return deep_reports
